{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_s28YzI8IBD"
   },
   "source": [
    "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\" width=\"300\"></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3N0Tchv8IBF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxCV20lq8IBH"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Типы функций потерь ($loss$) в задачах машинного обучения</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JImai3c78IBJ"
   },
   "source": [
    "Когда мы обучаем какой-либо алгоритм машинного обучения, мы должны задать как минимум 3 вещи:  \n",
    "\n",
    "- дать алгоритму **данные**\n",
    "- выбрать алгоритм **оптимизации**\n",
    "- выбрать **функцию потерь ($loss$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WQmQ-bp8IBK"
   },
   "source": [
    "Допустим, что данные уже есть и алгоритм оптимизации выбран. Рассмотрим типы задач машинного обучения и связанные с ними функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NjSX1_F8IBM"
   },
   "source": [
    "* Неформальное определение функции потерь:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZi9KpuX8IBN"
   },
   "source": [
    "***Функция потерь*** -- это функция, которая принимает на вход предсказания модели и истинные значения целевого признака, а возвращает неотрицательное число, характеризующее то, насколько хорошо наша модель предсказывает целевой признак. Функция потерь всегда зависит от задачи (точнее -- от типа предсказываемого признака)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ah5TjmuL8IBO"
   },
   "source": [
    "* Формальное определение:  \n",
    "\n",
    "Функция потерь - это функция $L: V \\times V \\to \\mathbb{R}^+$, где множество $V$ зависит от задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v-KCi0bD8IBP"
   },
   "source": [
    "Рассмотрим 3 задачи:  \n",
    "- задача регрессии ($y \\in \\mathbb{R}$)\n",
    "- бинарная классификация (на 2 класса)  \n",
    "- многоклассовая классификация (на $K$ классов)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dyxOuM98IBR"
   },
   "source": [
    "**Чем лучше предсказывает модель, тем меньше значеия функции потерь. Чем модель хуже, тем значения функции потерь больше.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rI7H6r48IBS"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Задача регрессии</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16hFIp2f8IBT"
   },
   "source": [
    "В задачах регрессии выделяют две основные функции потерь:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbZwAJ4D8IBU"
   },
   "source": [
    "1). **Квадратичная функция потерь**:  \n",
    "\n",
    "$$L(\\hat{y}, y) = (\\hat{y} - y)^2, ~~~~~\\hat{y}, y \\in \\mathbb{R}$$\n",
    "\n",
    "Если записывать для всей выборки $(X, y)$, где $X$ -- матрица объекты-принаки размера $(n, m)$, а y - столбец истинных ответов размера $(n, 1)$, то:\n",
    "\n",
    "$$L(\\hat{y}, y) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "-- в этом случае её ещё называют **среднеквадратичная ошибка (Mean Squared Error (MSE, MSELoss))**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZ1AXnsf8IBV"
   },
   "source": [
    "2). **Абсолютная функция потерь**:  \n",
    "\n",
    "$$L(\\hat{y}, y) = |\\hat{y} - y|, ~~~~~\\hat{y}, y \\in \\mathbb{R}$$\n",
    "\n",
    "Если записывать для всей выборки $(X, y)$, где $X$ -- матрица объекты-принаки размера $(n, m)$, а y - столбец истинных ответов размера $(n, 1)$, то:\n",
    "\n",
    "$$L(\\hat{y}, y) = \\frac{1}{n} \\sum_{i=1}^n |\\hat{y}_i - y_i|$$\n",
    "\n",
    "-- в этом случае её ещё называют **средняя абсолютная ошибка (Mean Absolute Error (MAE, MAELoss))**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tgs7Vfj68IBW"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Бинарная классификация</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9hQ8Gd5h8IBX"
   },
   "source": [
    "В задачах бинарной классификации иногда тоже используют среднеквадратичную ошибку, однако, например, когда в качестве модели будет выступать *нейрон с сигмоидой (= логистическая регрессия)*, будут некоторые проблемы (см. ноутбук [nn]neuron_logloss.ipynb), поэтому чаще всё-таки используют так называемый $LogLoss$ (или логистическая функция потерь):  \n",
    "\n",
    "$$L(\\hat{y}, y) = -\\frac{1}{n} \\sum_{i=1}^n y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAkv227Y8IBZ"
   },
   "source": [
    "(тут записано уже сразу по всей выборке)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0ydE2By8IBa"
   },
   "source": [
    "$LogLoss$ чаще всего используют в случае, когда в качестве $\\hat{y}_i$ мы предсказываем вероятность принадлежности к одному из классов, а $y_i \\in \\{0, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iykjmalr8IBb"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Многоклассовая классификация</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8zlZCvA8IBd"
   },
   "source": [
    "Сначала вспомним, как кодируют целевую переменную в случае многоклассовой классификации.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_KKnCuB8IBe"
   },
   "source": [
    "Изначально целевой переменной $y$ (классом, который хотим предсказать) может быть и вовсе какая-нибудь строка ('cat', 'dog', 'human'). Такие метки и признаки называют **категориальными**. В этом случае стоит закодировать $y$ просто последовательными неотрицательными числами:  \n",
    "\n",
    "'cat' $\\to$ 0  \n",
    "'dog' $\\to$ 1  \n",
    "'human' $\\to$ 2  \n",
    "\n",
    "В случае $K$ классов будет $K$ различных числовых значений. Это называется ***LabelEncoding*** (перевод категорий в числовые метки)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YoEG2nEI8IBh"
   },
   "source": [
    "Однако заметим, что такое LabelEncoding задаёт на классах **отношение порядка**, то есть мы, например, можем сказать, что 'human' (2) > 'cat' (0) в данном случае. Это неправильно, поскольку это сравнение некорректно и может привести к непредсказуемымому результату."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Ye6P0DZ8IBi"
   },
   "source": [
    "Поэтому после LabelEncoding'а всегда применяют ***OneHotEncoding***:  \n",
    "\n",
    "0 $\\to$ (1, 0, 0)  \n",
    "1 $\\to$ (0, 1, 0)  \n",
    "2 $\\to$ (0, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi5vxEJ_8IBj"
   },
   "source": [
    "То есть преобразуют числовое значение класса в строку размера $(1, K)$, где $K$ -- количество классов (в данном случае их 3, поэтому строка длины 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pu-CD_MU8IBl"
   },
   "source": [
    "Тогда предобработка категориальной метки  $y$ такая: \n",
    "- изначальный **столбец меток $y$ размера $(n, 1)$** переходит в **столбец чисел $y_{LabelEncoded}$ размера $(n, 1)$** \n",
    "- столбец $y_{LabelEncoded}$ переходит **в матрицу из 0 и 1 $y_{OneHotEncoded}$ размера $(n, K)$** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8LjmWam8IBp"
   },
   "source": [
    "С $\\hat{y}$ -- предсказаниями модели -- та же ситуация, только обычно предсказываются сначала вероятности принадлежности к каждому из классов, а потом уже среди них берётся наибольшая и на её месте ставится 1 в строке, а на месте остальных вероятностей ставится 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqbcQfBt8IBr"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qz0n44hS8IBv"
   },
   "source": [
    "*Небольшое отступление про Softmax*: как, имея строку значений $(a_1, a_2, ..., a_K)$, привести все их значения к диапазону $[0, 1]$ так, чтобы в сумме они давали единицу (то есть чтобы можно было интерпретровать значения как вероятности)? Для этого существует (один из способов) функция под названием $Softmax$:  \n",
    "\n",
    "$$Softmax((a_1, a_2, ..., a_K))_i = \\frac{e^{a_i}}{\\sum_{j=1}^K e^{a_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXNDj2Gi8IBz"
   },
   "source": [
    "**Упражение:** убедитесь, что сумма этих значений будет давать в сумме единицу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvAvDQse8IB2"
   },
   "source": [
    "То есть строка $(a_1, a_2, ..., a_K)$ переходит в строку:  \n",
    "\n",
    "$$(a_1, a_2, ..., a_K) \\to \\left(\\frac{e^{a_1}}{\\sum_{j=1}^K e^{a_j}}, \\frac{e^{a_2}}{\\sum_{j=1}^K e^{a_j}}, ..., \\frac{e^{a_K}}{\\sum_{j=1}^K e^{a_j}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNH8vr778IB3"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95U2Q_bh8IB4"
   },
   "source": [
    "В задаче многоклассовой классификации чаще всего пользуются функцией под названиаем $CrossEntropy$ -- кросс-энтропия:  \n",
    "\n",
    "$$L(\\hat{y}, y) = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^K y_{ij} \\log(\\hat{y_{ij}})$$  \n",
    "\n",
    "где $y$ -- матрица $(n, K)$ истинных значений классов объектов (закодированная OneHot'ом), а $\\hat{y}$ -- матрица $(n, K)$ предсказаний модели ($\\hat{y}_{ij}$ -- \"вероятность\" принадлежности $i$-го объекта к $j$-му классу)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EE9rqaND8IB5"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Важно</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsZcpo1J8IB6"
   },
   "source": [
    "В данном ноутбуке перечислены далеко не все функции потерь и описаны далеко не все задачи машинного обучения, а лишь 3 конкретных. Более подробно про функции потерь в других задачах и об ML в целом можно больше узнать на курсах, таких, как, например, [курс Воронцова К.В.](https://www.youtube.com/playlist?list=PLJOzdkh8T5kp99tGTEFjH_b9zqEQiiBtC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0njZv2u8IB7"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAINFtXX8IB8"
   },
   "source": [
    "1). [Вики-страничка по функциям потерь](http://ru.learnmachinelearning.wikia.com/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C_(Loss_function) (информации немного, но зато упомянуты другие функции потерь)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUtqypey8IB9"
   },
   "source": [
    "2). Github с очень большим количеством полезных материалов по Machine Learning: https://github.com/demidovakatya/vvedenie-mashinnoe-obuchenie"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]loss_functions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
