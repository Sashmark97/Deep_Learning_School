{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[homework]gradient_linear_model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"EvkV_R3vzl_m","colab_type":"text"},"cell_type":"markdown","source":["<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\"  width=400></p>\n","\n","<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"]},{"metadata":{"id":"rbRbOixxzl_n","colab_type":"text"},"cell_type":"markdown","source":["---"]},{"metadata":{"id":"4mYrykelzl_o","colab_type":"text"},"cell_type":"markdown","source":["<h2 style=\"text-align: center;\"><b>Градиентный спуск и линейные модели: домашнее задание</b></h2>"]},{"metadata":{"id":"K1xoftyezl_p","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import scipy\n","from matplotlib import pylab, gridspec, pyplot as plt\n","\n","%matplotlib inline\n","plt.style.use('fivethirtyeight')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yY2kUgovzl_s","colab_type":"text"},"cell_type":"markdown","source":["### 1. Градиентный спуск: повторение"]},{"metadata":{"id":"ctYan0Tfzl_t","colab_type":"text"},"cell_type":"markdown","source":["Рассмотрим функцию от двух переменных:"]},{"metadata":{"id":"xDCA6t7Azl_v","colab_type":"code","colab":{}},"cell_type":"code","source":["def f(x1, x2):\n","    return np.sin(x1)**2 + np.sin(x2) ** 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uV1aB-CQzl_w","colab_type":"text"},"cell_type":"markdown","source":["***Reminder:***  \n","Что мы хотим? Мы хотим найти минимум этой функции (в машинному обучении мы обычно хотим найти минимум **функции потерь**, например, MSE), а точнее найти x1 и x2 такие, что при них значение f(x1,x2) минимально, то есть *точку экстремума*.  \n","Как мы будем искать эту точку? Используем методы оптимизации (=минимизации в нашем случае). Одним из таких методов и является градиентный спуск. "]},{"metadata":{"id":"7b185-dEzl_x","colab_type":"text"},"cell_type":"markdown","source":["Реализуем функцию, которая будет осуществлять градиентный спуск для функции f:"]},{"metadata":{"id":"lsnvRASDkYXi","colab_type":"text"},"cell_type":"markdown","source":["*Примечание:* Вам нужно посчитать частные производные именно **аналитически** и **переписать их в код**, а не считать производные численно (через отношение приращения функции к приращению аргумента) -- в этих двух случаях могут различаться ответы, поэтому будьте внимательны"]},{"metadata":{"id":"VfgA69Vizl_y","colab_type":"code","colab":{}},"cell_type":"code","source":["def grad_descent(lr, num_iter=100):\n","    \"\"\"\n","    функция, которая реализует градиентный спуск в минимум для функции f от двух переменных. \n","        param lr: learning rate алгоритма\n","        param num_iter: количество итераций градиентного спуска\n","    \"\"\"\n","    global f\n","    # в начале градиентного спуска инициализируем значения x1 и x2 какими-нибудь числами\n","    cur_x1, cur_x2 = 1.5, -1\n","    # будем сохранять значения аргументов и значений функции в процессе град. спуска в переменную states\n","    steps = []\n","    \n","    # итерация цикла -- шаг градиентнго спуска\n","    for iter_num in range(num_iter):\n","        steps.append([cur_x1, cur_x2, f(cur_x1, cur_x2)])\n","        \n","        # чтобы обновить значения cur_x1 и cur_x2, как мы помним с последнего занятия, \n","        # нужно найти производные (градиенты) функции f по этим переменным.\n","        grad_x1 = <тут Ваш код>\n","        grad_x2 = <тут Ваш код>\n","                 \n","        # после того, как посчитаны производные, можно обновить веса. \n","        # не забудьте про lr!\n","        cur_x1 -= <тут Ваш код>\n","        cur_x2 -= <тут Ваш код>\n","    return np.array(steps)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JKffGXNrzl_0","colab_type":"text"},"cell_type":"markdown","source":["Запустим градиентный спуск:"]},{"metadata":{"scrolled":true,"id":"XVtijv6Dzl_1","colab_type":"code","colab":{}},"cell_type":"code","source":["steps = grad_descent(lr=0.5, num_iter=10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GKaPajd6zl_4","colab_type":"text"},"cell_type":"markdown","source":["Визуализируем точки градиентного спуска на 3D-графике нашей функции. Звездочками будут обозначены точки (тройки x1, x2, f(x1, x2)), по которым Ваш алгоритм градиентного спуска двигался к минимуму.\n","\n","(Для того, чтобы написовать этот график, мы и сохраняли значения cur_x1, cur_x2, f(cur_x1, cur_x2) в steps в процессе спуска)"]},{"metadata":{"id":"JODF87Ilzl_4","colab_type":"text"},"cell_type":"markdown","source":["Если у Вас правильно написана функция grad_descent, то звездочки на картинке должны сходиться к одной из точку минимума функции. Вы можете менять начальные приближения алгоритма, значения lr и num_iter и получать разные результаты."]},{"metadata":{"id":"Y2PqLNfEzl_5","colab_type":"code","colab":{}},"cell_type":"code","source":["# %matplotlib osx\n","\n","from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","import numpy as np\n","\n","path = []\n","\n","X, Y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n","\n","fig = plt.figure(figsize=(16, 10))\n","ax = fig.gca(projection='3d')\n","\n","ax.plot(xs=steps[:, 0], ys=steps[:, 1], zs=steps[:, 2], marker='*', markersize=20,\n","                markerfacecolor='y', lw=3, c='black')\n","\n","ax.plot_surface(X, Y, f(X, Y), cmap=cm.coolwarm)\n","ax.set_zlim(0, 5)\n","ax.view_init(elev=60)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FwaDZNzzzl_8","colab_type":"text"},"cell_type":"markdown","source":["#### Ответ на КР  №1"]},{"metadata":{"id":"0PoHtXI4zl_9","colab_type":"text"},"cell_type":"markdown","source":["Запустите Вашу функцию grad_descent c параметрами lr=0.3, num_iter=20 и начальными приближениями  cur_x1, cur_x2 = 1.5, -1. Сумма значений x1, x2 и f(x1, x2) (т.е. сумма значений в steps[-1]), умноженная на 10\\**6 и округленная до 2 знаков после запятой, будет ответом на первый вопрос кр."]},{"metadata":{"id":"IMOAWHEBzl_-","colab_type":"code","colab":{}},"cell_type":"code","source":["steps = grad_descent(<тут Ваш код>)\n","np.sum(steps[-1]) * 10**6"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vvgaD86KzmAC","colab_type":"text"},"cell_type":"markdown","source":["### 2. Линейные модели"]},{"metadata":{"id":"tQ50iMzwzmAC","colab_type":"text"},"cell_type":"markdown","source":["Возьмем код для линейной регресси с семинара. Напомним, что найти веса W и b для линейной регресси можно двумя способами: обращением матриц (функция solve_weights) и градиентным спуском (функция grad_descent). Мы здесь будем рассматривать градиентный спуск."]},{"metadata":{"id":"3Mm7F1rizmAD","colab_type":"code","colab":{}},"cell_type":"code","source":["W = None\n","b = None\n","\n","def mse(preds, y):\n","    return ((preds - y)**2).mean()\n","    \n","def grad_descent(X, y, lr, num_iter=100):\n","    global W, b\n","    np.random.seed(40)\n","    W = np.random.rand(X.shape[1])\n","    b = np.array(np.random.rand(1))\n","    \n","    losses = []\n","    \n","    N = X.shape[0]\n","    for iter_num in range(num_iter):\n","        preds = predict(X)\n","        losses.append(mse(preds, y))\n","        \n","        w_grad = np.zeros_like(W)\n","        b_grad = 0\n","        for sample, prediction, label in zip(X, preds, y):\n","            w_grad += 2 * (prediction - label) * sample\n","            b_grad += 2 * (prediction - label)\n","            \n","        W -= lr * w_grad\n","        b -= lr * b_grad\n","    return losses\n","\n","def predict(X):\n","    global W, b\n","    return np.squeeze(X @ W + b.reshape(-1, 1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bYKGc0aHzmAG","colab_type":"text"},"cell_type":"markdown","source":["Рассмотрим функцию:  \n","\n","$$f(x, y) = 0.43x+0.5y + 0.67$$  "]},{"metadata":{"id":"kQXFf_Y9zmAH","colab_type":"text"},"cell_type":"markdown","source":["***Friendly reminder:***  \n","Что мы хотим? Мы хотим уметь \"восстанавливать функцию\" -- то есть предсказывать значения функции в точках с координатами (x, y) (именно так и получается 3D-график -- (x, y, f(x,y)) в пространстве).  \n","В чём сложность? Нам дан только конечный небольшой набор точек (30 в данном случае), по которому мы хотим восстановить зависимость, по сути, непрерывную. Линейная регрессия как раз подходит для восстановления линейной зависимости (а у нас функция сейчас как раз линейная -- только сложение аргументов и умножение на число)."]},{"metadata":{"id":"sYO-jgNszmAK","colab_type":"text"},"cell_type":"markdown","source":["Cгерерируем шумные данные из этой функции (как на семинаре):"]},{"metadata":{"id":"qKjm2cQrzmAK","colab_type":"code","colab":{}},"cell_type":"code","source":["np.random.seed(40)\n","func = lambda x, y: (0.43*x + 0.5*y + 0.67 + np.random.normal(0, 7, size=x.shape))\n","\n","X = np.random.sample(size=(30)) * 10\n","Y = np.random.sample(size=(30)) * 150\n","result_train = [func(x, y) for x, y in zip(X, Y)]\n","data_train = np.concatenate([X.reshape(-1, 1), Y.reshape(-1, 1)], axis=1)\n","\n","pd.DataFrame({'x': X, 'y': Y, 'res': result_train}).head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ETB66yWyzmAN","colab_type":"text"},"cell_type":"markdown","source":["Посмотрим, что же мы сгенерировали:"]},{"metadata":{"id":"DUXvT674zmAN","colab_type":"code","colab":{}},"cell_type":"code","source":["# %matplotlib osx\n","\n","fig = plt.figure(figsize=(16, 10))\n","ax = fig.gca(projection='3d')\n","X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 150, 100))\n","ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n","ax.plot_surface(X,Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n","\n","ax.view_init(elev=60)\n","plt.ion()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"09leW_BazmAQ","colab_type":"text"},"cell_type":"markdown","source":["Теперь давайте попробуем применить к этим данным нашу линейную регрессию и с помощью неё предсказать истинный график функции:"]},{"metadata":{"id":"XXYMvQxfzmAR","colab_type":"code","colab":{}},"cell_type":"code","source":["losses = grad_descent(data_train, result_train, 1e-2, 5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UITmWCUNzmAT","colab_type":"code","colab":{}},"cell_type":"code","source":["W, b"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P8fA0-DczmAV","colab_type":"text"},"cell_type":"markdown","source":["Посмотрим на график лосса:"]},{"metadata":{"id":"_LPNor1fzmAW","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.plot(losses), losses[-1];"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sVYB8qUOzmAZ","colab_type":"text"},"cell_type":"markdown","source":["И на полученную разделяющую плоскость:"]},{"metadata":{"id":"gMdpNJM6zmAa","colab_type":"code","colab":{}},"cell_type":"code","source":["# %matplotlib osx\n","\n","fig = plt.figure(figsize=(16, 10))\n","ax = fig.gca(projection='3d')\n","X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 150, 100))\n","ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n","ax.plot_surface(X,Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n","ax.plot_surface(X,Y, W[0]*X + W[1]*Y + b, color='blue', alpha=0.3)\n","\n","ax.view_init(elev=60)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sFczLgyuzmAe","colab_type":"text"},"cell_type":"markdown","source":["(зелёная плоскость -- истинная функция, синяя плоскость -- предсказание)"]},{"metadata":{"id":"0BzdivVFzmAf","colab_type":"text"},"cell_type":"markdown","source":["Охх, лосс и коэффициенты (W и b) нашей модели быстро уходит в небеса, и график предсказан неправильно. Почему такое происходит?\n","\n","В данном случае дело в том, что признаки имеют разный *масштаб* (посмотрите на значения X и Y -- они лежат в разных диапазонах). Многие модели машинного обучения, в том числе линейные, будут плохо работать в таком случае (на самом деле это зависит от метода оптимизации, сейчас это градиентный спуск).  \n","\n","Есть несколько способов **масштабирования**:\n","1. **Нормализация (она же стандартизация, StandardScaling)**:  \n","\n","$$x_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}$$  \n","(j -- номер признака, i -- номер объекта)  \n","То есть вычитаем среднее по столбцу и делим на корень из дисперсии.\n","\n","2. **Приведение к отрезку [0,1] (MinMaxScaling)**:  \n","\n","$$x_{ij} = \\frac{x_{ij} - \\min_j}{\\max_j - \\min_j}$$  \n","(j -- номер признака, i -- номер объекта)  \n","То есть вычитаем минимум по столбцу и делим на разницу между минимумом и максимумом."]},{"metadata":{"id":"oOXgLIWTzmAg","colab_type":"text"},"cell_type":"markdown","source":["Используем нормализацию:"]},{"metadata":{"id":"4gOqi21ezmAh","colab_type":"text"},"cell_type":"markdown","source":["### 2.1 Нормализация"]},{"metadata":{"id":"XfoQbSEyzmAi","colab_type":"text"},"cell_type":"markdown","source":["Посмотрим на среднее и разброс значений в признаках (координатах) до масштабирования:"]},{"metadata":{"id":"npJ9aGzZzmAi","colab_type":"code","colab":{}},"cell_type":"code","source":["data_train.mean(axis=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SghPwBF3zmAk","colab_type":"code","colab":{}},"cell_type":"code","source":["data_train.std(axis=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lZcvJRf1zmAm","colab_type":"text"},"cell_type":"markdown","source":["То есть в первом столбце у нас среднее 5.6 и среднеквадратичное отклонение 2.7, а во втором столбце среднее 70.8 и среднеквадратичное отклонение 45.3.  \n","\n","Будьте внимательны: среднеквадратичное отклонение НЕ говорит о том, какой *максимальный* разброс, оно лишь указывает на числовой интервал, вероятность попадания в который у данного признака высока (то есть, например, в интервал [2.9, 8.3] в первом случае). Для большей информации смотрите [среднеквадратичное отклонение](https://ru.wikipedia.org/wiki/%D0%A1%D1%80%D0%B5%D0%B4%D0%BD%D0%B5%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BE%D1%82%D0%BA%D0%BB%D0%BE%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5) и [доверительные интервалы](https://ru.wikipedia.org/wiki/%D0%94%D0%BE%D0%B2%D0%B5%D1%80%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%B2%D0%B0%D0%BB)\n","\n","Однако факт в том, что масштаб у этих признаков разный, давайте исправим это."]},{"metadata":{"id":"I0DMbY5yzmAn","colab_type":"text"},"cell_type":"markdown","source":["**Нормализуйте признаки** так, чтобы среднее значение в каждом столбце было ~0, а стандартное отклонение ~1:"]},{"metadata":{"id":"by_okjU0zmAo","colab_type":"code","colab":{}},"cell_type":"code","source":["data_train_normalized = <тут Ваш код>"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Hp-JLFDRzmAr","colab_type":"text"},"cell_type":"markdown","source":["Проверьте средние и диспресию (.std()):"]},{"metadata":{"id":"XUFuFHrmzmAs","colab_type":"code","colab":{}},"cell_type":"code","source":["<тут Ваш код>"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cOa4B4IXzmAu","colab_type":"code","colab":{}},"cell_type":"code","source":["<тут Ваш код>"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kmFr422fzmAy","colab_type":"text"},"cell_type":"markdown","source":["Попробуем теперь запустить регрессию с теми же параметрами lr и num_iter:"]},{"metadata":{"id":"0QbFEV7ZzmAy","colab_type":"code","colab":{}},"cell_type":"code","source":["losses = grad_descent(data_train_normalized, result_train, 1e-2, 100)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tVruzNtAzmA0","colab_type":"code","colab":{}},"cell_type":"code","source":["W, b"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vn4NXpo4zmA2","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.plot(losses)\n","print(losses[-1]);"],"execution_count":0,"outputs":[]},{"metadata":{"id":"de8U-uSYzmA4","colab_type":"text"},"cell_type":"markdown","source":["Мы видим, что теперь коэффициенты по модулю не огромны, градиентный спуск не взрывается и лосс стабилен!"]},{"metadata":{"id":"-wLnnMekzmA5","colab_type":"text"},"cell_type":"markdown","source":["Посмотрим на полученную плоскость:"]},{"metadata":{"id":"ZGEhS_nvzmA5","colab_type":"code","colab":{}},"cell_type":"code","source":["# %matplotlib osx\n","\n","fig = plt.figure(figsize=(16, 10))\n","ax = fig.gca(projection='3d')\n","\n","ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n","\n","X, Y = np.meshgrid(np.linspace(-1, 10, 100), np.linspace(-1, 150, 100))\n","ax.plot_surface(X, Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n","\n","X, Y = np.meshgrid(np.linspace(-1, 10, 100), np.linspace(-1, 150, 100))\n","# Hint: не забудьте нормализовать X и Y тоже\n","ax.plot_surface(X, Y, W[0] * <тут Ваш код> + W[1] * <тут Ваш код> + b, color='blue', alpha=0.3)\n","\n","ax.view_init(elev=60)\n","plt.ion()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UVR-nmjszmA7","colab_type":"text"},"cell_type":"markdown","source":["#### Ответ на КР  №2"]},{"metadata":{"id":"H2j4xKJ3zmA8","colab_type":"text"},"cell_type":"markdown","source":["Запустите градиентный спуск на нормализованных данных с параметрами lr=1e-2, num_iter=200 и посчитайте сумму последних трех значений лосса. Полученное число, округленное до второго знака после запятой, будет ответом на второе задание кр."]},{"metadata":{"id":"n-R6jefyzmA8","colab_type":"code","colab":{}},"cell_type":"code","source":["losses = grad_descent(<тут Ваш код>)\n","np.sum(losses[-3:])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LnJmxSlCzmA-","colab_type":"text"},"cell_type":"markdown","source":["### 2.2 Регуляризация"]},{"metadata":{"id":"A0NXw5MtzmA_","colab_type":"text"},"cell_type":"markdown","source":["Помимо \"сырой\" линейной регресси часто используют линейную регрессию с регуляризацией -- Lasso или Ridge регрессию. Они отличаются только типом \"штрафа\" за большие веса: учитывать модули (Lasso) или квадраты весов (Ridge)."]},{"metadata":{"id":"99ajXzw8zmA_","colab_type":"text"},"cell_type":"markdown","source":["Учитывая, что наш лосс в этой задаче -- Mean Squared Error (MSE), в случае Ridge-регресси будет:  \n","\n","$$Loss = MSE = \\sum (pred_i-y_i)^2 + \\alpha*\\sum W_i^2$$  \n","\n","А в случае Lasso-регресси будет:  \n","\n","$$Loss = MSE = \\sum (pred_i-y_i)^2 + \\alpha*\\sum |W_i|$$  \n","\n","\n","Здесь $\\alpha$ -- заранее задаваемый гиперпараметр. Это вес, с которым второе слагаемое будет влиять на лосс."]},{"metadata":{"id":"-_w_qDo2zmBA","colab_type":"text"},"cell_type":"markdown","source":["Добавление регуляризации, как правило, помогает бороться с **переобучением**."]},{"metadata":{"id":"b0Ggld74zmBB","colab_type":"text"},"cell_type":"markdown","source":["Подробнее об этом можно почитать [тут](http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%B0%D1%81%D1%81%D0%BE) и [тут](http://www.machinelearning.ru/wiki/index.php?title=%D0%A0%D0%B8%D0%B4%D0%B6-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F)."]},{"metadata":{"id":"X-5_oOJCzmBB","colab_type":"text"},"cell_type":"markdown","source":["### Полезные ссылки:\n","1. Открытый курс по машинному обучению: https://habr.com/company/ods/blog/323890/\n","2. Если вам интересно математическое обоснование всех методов, рекомендуем ознакомиться с этой книгой: https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf"]}]}